{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import collections\n",
    "\n",
    "# Change dir for custom imports\n",
    "os.chdir('../')\n",
    "dataset = 'epinions'\n",
    "dataset_path = os.path.join('datasets', dataset, 'v1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data (If previously computed, skip this and load the file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trust data\n",
    "f = open(os.path.join(dataset_path, 'user_rating.txt'), 'r') # open the file for reading\n",
    "raw_data = []\n",
    "for row_num, line in enumerate(f):\n",
    "    values = line.strip().split('\\t')\n",
    "    raw_data.append([v for v in values])\n",
    "trust_data = np.array(raw_data)\n",
    "f.close()\n",
    "\n",
    "trust_raw_df = pd.DataFrame(trust_data).rename(columns={\n",
    "    0: 'truster',\n",
    "    1: 'trusted',\n",
    "    2: 'value',\n",
    "    3: 'date'\n",
    "})\n",
    "\n",
    "# Fix datatypes\n",
    "trust_raw_df = trust_raw_df.astype({'truster':'int64', 'trusted':'int64', 'value':'int'})\n",
    "\n",
    "# Select only positive trust to build the communities\n",
    "trust_raw_df = trust_raw_df[trust_raw_df['value'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the original epinions txt file\n",
    "f = open(os.path.join(dataset_path, 'rating.txt'), 'r') # open the file for reading\n",
    "data = []\n",
    "for row_num, line in enumerate(f):\n",
    "    values = line.strip().split('\\t')\n",
    "    data.append([v for v in values])\n",
    "\n",
    "    if row_num == 6000000:\n",
    "        break\n",
    "rating_data = np.array(data)\n",
    "f.close()\n",
    "\n",
    "# Update the datatypes\n",
    "ratings_df = pd.DataFrame(rating_data).rename(columns={\n",
    "    0: 'itemID',\n",
    "    1: 'userID',\n",
    "    2: 'rating',\n",
    "    3: 'status'\n",
    "})\n",
    "# Fix datatypes\n",
    "ratings_df = ratings_df.astype({'itemID':'int64', 'userID':'int64', 'status':'int', 'rating':'int'})\n",
    "\n",
    "# Dataset stats\n",
    "print(f\"Total dataset users: {len(set(ratings_df.userID.to_list()))}\")\n",
    "print(f\"Total dataset ratings: {len(ratings_df.userID.to_list())}\")\n",
    "\n",
    "# Save the original dataset (to match the users selected with the social aspect in the upcoming experiment)\n",
    "# ratings_df.to_csv('./datasets/epinions/v1/ratings_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter records that have at least I total ratings and U total users\n",
    "I = 150\n",
    "U = 100\n",
    "\n",
    "# Items filter\n",
    "grouped_item_ratings = ratings_df.groupby(by=\"itemID\")[\"rating\"].count()\n",
    "selected_items = grouped_item_ratings[grouped_item_ratings > I].index.tolist()\n",
    "df = ratings_df[ratings_df['itemID'].isin(selected_items)]\n",
    "\n",
    "# Users filter\n",
    "grouped_user_ratings = df.groupby(by=\"userID\")[\"rating\"].count()\n",
    "selected_users = grouped_user_ratings[grouped_user_ratings > U].index.tolist()\n",
    "df = df[df['userID'].isin(selected_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users list\n",
    "users_list = list(set(df.userID.to_list()))\n",
    "\n",
    "# Select trust data for users that appear in our selected sub-dataset\n",
    "df_trust = trust_raw_df[(trust_raw_df['truster'].isin(users_list)) & (trust_raw_df['trusted'].isin(users_list))]\n",
    "\n",
    "# Get all the unique users\n",
    "users_df = df[['userID']].drop_duplicates()\n",
    "df_trust = users_df.merge(df_trust, left_on='userID', right_on='truster')\n",
    "df_trust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the adjacent matrix from trust data to calculate the communities\n",
    "adj_matrix = pd.crosstab(df_trust.truster, df_trust.trusted)\n",
    "idx = adj_matrix.columns.union(adj_matrix.index)\n",
    "adj_matrix = adj_matrix.reindex(index = idx, columns=idx, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from communities.algorithms import girvan_newman\n",
    "\n",
    "communities, _ = girvan_newman(adj_matrix.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(communities[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.read_csv('./datasets/epinions/v1/ratings_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply dataset filtering and create the features for Clustering (If pre-computed, just load the file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter records that have at least I total ratings and U total users\n",
    "I = 150\n",
    "U = 100\n",
    "\n",
    "# Items filter\n",
    "grouped_item_ratings = ratings_df.groupby(by=\"itemID\")[\"rating\"].count()\n",
    "selected_items = grouped_item_ratings[grouped_item_ratings > I].index.tolist()\n",
    "df = ratings_df[ratings_df['itemID'].isin(selected_items)]\n",
    "\n",
    "# Users filter\n",
    "grouped_user_ratings = df.groupby(by=\"userID\")[\"rating\"].count()\n",
    "selected_users = grouped_user_ratings[grouped_user_ratings > U].index.tolist()\n",
    "df = df[df['userID'].isin(selected_users)]\n",
    "\n",
    "# reset numbers to avoid long values and clean columns\n",
    "df['userID2'] = pd.factorize(df['userID'])[0]\n",
    "df['itemID2'] = pd.factorize(df['itemID'])[0]\n",
    "df.drop(['status', 4, 5, 6, 7], axis=1, inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df = df[['userID2', 'itemID2', 'rating']].rename(columns={'userID2': 'userID', 'itemID2': 'itemID'})\n",
    "\n",
    "# Dataset stats\n",
    "print(f\"Total dataset users: {len(set(df.userID.to_list()))}\")\n",
    "print(f\"Total dataset ratings: {len(df.userID.to_list())}\")\n",
    "\n",
    "# Save the dataset for training in the second experiment\n",
    "# df.to_csv('./datasets/epinions/v1/ratings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/epinions/v1/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating threshold for +ve and -ve ratings features\n",
    "r_th = 4\n",
    "\n",
    "# Get all the unique users\n",
    "users_df = df[['userID']].drop_duplicates()\n",
    "\n",
    "# Create the possible features\n",
    "user_total_ratings = df.groupby(by=\"userID\")[\"rating\"].count()\n",
    "user_positive_ratings = df[df['rating'] > r_th].groupby('userID')['rating'].count().reset_index()\n",
    "user_negative_ratings = df[df['rating'] <= r_th].groupby('userID')['rating'].count().reset_index()\n",
    "\n",
    "# Update main df\n",
    "users_df[\"ratings\"] = user_total_ratings.values\n",
    "users_df = pd.merge(users_df, user_positive_ratings, on=['userID'], how='left').rename(columns={'rating':'positive_ratings'})\n",
    "users_df = pd.merge(users_df, user_negative_ratings, on=['userID'], how='left').rename(columns={'rating':'negative_ratings'})\n",
    "\n",
    "# # Clean none values\n",
    "users_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gower\n",
    "\n",
    "# Get the gower distance matrix\n",
    "distance_matrix = gower.gower_matrix(users_df.drop(columns=['userID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=shc.ClusterWarning)\n",
    "    # Compute linkage using the distance matrix\n",
    "    linkage = shc.linkage(\n",
    "        distance_matrix,\n",
    "        method='ward'\n",
    "    )\n",
    "\n",
    "# Use fcluster to get the cluster labels\n",
    "# `t` is the threshold to use to cut the dendrogram - higher `t` means less clusters / more data points within individual clusters\n",
    "t = 6\n",
    "clusters = shc.fcluster(linkage, t, criterion='distance')\n",
    "\n",
    "# get unique cluster labels\n",
    "unique_labels = np.unique(clusters)\n",
    "\n",
    "# Adding the results to a new column in the dataframe\n",
    "users_df[\"cluster_shc\"] = clusters\n",
    "\n",
    "print(f'Generated {len(unique_labels)} clusters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter(users_df.cluster_shc.to_list())\n",
    "print(counter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize clusters (theoretical - Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a t-SNE object\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30, # [5, 50] default is 30\n",
    "    learning_rate=200, # [10.0, 1000.0] , def=200\n",
    "    n_iter=1000, # >250, def=1000\n",
    "    metric=\"euclidean\"\n",
    ")\n",
    "\n",
    "# Perform t-SNE on the distance matrix\n",
    "tsne_data = tsne.fit_transform(distance_matrix)\n",
    "# test = tsne.fit(distance_matrix)\n",
    "\n",
    "# Plot the t-SNE data using a scatter plot\n",
    "plt.scatter(tsne_data[:, 0], tsne_data[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cluster labels to the t-SNE data\n",
    "tsne_data_clusters = np.column_stack((tsne_data, clusters))\n",
    "\n",
    "# Plot the t-SNE data using a scatter plot\n",
    "plt.scatter(tsne_data_clusters[:, 0], tsne_data_clusters[:, 1], c=tsne_data_clusters[:, 2], cmap='Spectral')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create groups of the user clusters formed in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the gower distance matrix\n",
    "distance_matrix_2 = gower.gower_matrix(users_df.drop(columns=['userID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=shc.ClusterWarning)\n",
    "    # Compute linkage using the distance matrix\n",
    "    linkage = shc.linkage(\n",
    "        distance_matrix_2,\n",
    "        method='ward'\n",
    "    )\n",
    "\n",
    "# Use fcluster to get the cluster labels\n",
    "# `t` is the threshold to use to cut the dendrogram - higher `t` means less clusters / more data points within individual clusters\n",
    "t = 4\n",
    "clusters = shc.fcluster(linkage, t, criterion='distance')\n",
    "\n",
    "# get unique cluster labels\n",
    "unique_labels = np.unique(clusters)\n",
    "\n",
    "# Adding the results to a new column in the dataframe\n",
    "users_df[\"group_clusters\"] = clusters\n",
    "\n",
    "print(f'Generated {len(unique_labels)} clusters.')\n",
    "counter = collections.Counter(users_df.group_clusters.to_list())\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the clusters created\n",
    "users_df.to_csv('./output/exp-3-epinions/clusters/clusters.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters-Serendipity iterations experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from models.lightgcn.train_clusters_script_epinions import train_on_groups\n",
    "\n",
    "# Baseline serendipity calculated from a normal train in a previous experiment on the dataset\n",
    "baseline_serendipity = pd.read_csv('./output/exp-3-epinions/baseline_serendipity.csv')\n",
    "\n",
    "iterations = np.linspace(4, 5, 2)\n",
    "for i in iterations:\n",
    "    clusters = shc.fcluster(linkage, i, criterion='distance')\n",
    "    # get unique cluster labels\n",
    "    unique_labels = np.unique(clusters)\n",
    "\n",
    "    # Total groups obtained (groups of clusters)\n",
    "    print(len(unique_labels))\n",
    "\n",
    "    # Save clusters in df\n",
    "    users_df[\"group_clusters\"] = clusters\n",
    "\n",
    "    # Train model and check serendipity per group\n",
    "    new_user_serendipity = train_on_groups(users_df)\n",
    "\n",
    "    # Apply condition (if we increased serendipity per 0% of groups stop and save i)\n",
    "    serendipity_df = baseline_serendipity.merge(new_user_serendipity, on='userID')\n",
    "    serendipity_df['comparison'] = serendipity_df.apply(lambda x: 1 if (x.user_serendipity_y > x.user_serendipity_x) else 0, axis=1)\n",
    "    total_users = len(serendipity_df)\n",
    "    total_hits = sum(serendipity_df.comparison.to_list())\n",
    "\n",
    "    threshold = total_hits/total_users * 100\n",
    "    print(threshold)\n",
    "    serendipity_df.to_csv('./output/exp-3-epinions/group_iterations_500k/' + str(i) + '.csv', index=False)\n",
    "\n",
    "    if threshold > 80:\n",
    "        print(\"Threshold achieved at\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "serendipity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
