{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import collections\n",
    "\n",
    "# change dir for custom imports\n",
    "os.chdir('../')\n",
    "from scripts.dataset_downloader import initialize_dataset\n",
    "from scripts.k_means import create_clsuters, clustering_errors\n",
    "from scripts.helpers import get_most_rated_movies\n",
    "\n",
    "\n",
    "dataset = 'ml-100k'\n",
    "dataset_path = os.path.join('datasets', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If dataset hasn't been previously initialized, it can be done with this function\n",
    "initialize_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_path = os.path.join(dataset_path, 'u.data')\n",
    "items_path = os.path.join(dataset_path, 'u.item')\n",
    "genres_path = os.path.join(dataset_path, 'u.genre')\n",
    "users_path = os.path.join(dataset_path, 'u.user')\n",
    "\n",
    "ratings_file = codecs.open(ratings_path, 'rU', 'UTF-8')\n",
    "items_file = codecs.open(items_path, 'rU', 'latin-1')\n",
    "users_file = codecs.open(users_path, 'rU', 'latin-1')\n",
    "\n",
    "# load data\n",
    "ratings_df = pd.read_csv(ratings_file, sep='\\t', names=('user_id', 'item_id', 'rating', 'timestamp'))\n",
    "genres_df = pd.read_csv(genres_path, sep='|', names=('title', 'id'))\n",
    "\n",
    "cols_names = ('id', 'title', 'year', 'nan', 'link') + tuple(genres_df.title.to_list())\n",
    "items_df = pd.read_csv(items_file, sep='|', usecols=list(range(0,24)), names=cols_names).drop(columns=['nan', 'link'])\n",
    "\n",
    "users_df = pd.read_csv(users_file, sep='|', usecols=list(range(0,4)), names=('user_id', 'age', 'gender', 'occupation'))\n",
    "\n",
    "# dataset stats\n",
    "print(f\"Total dataset users: {len(set(ratings_df.user_id.to_list()))}\")\n",
    "print(f\"Total dataset ratings: {len(ratings_df.user_id.to_list())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_total_ratings = ratings_df.groupby(by=\"user_id\")[\"rating\"].count()\n",
    "user_positive_ratings = ratings_df[ratings_df['rating'] > 3].groupby('user_id')['rating'].count().reset_index()\n",
    "user_negative_ratings = ratings_df[ratings_df['rating'] <= 3].groupby('user_id')['rating'].count().reset_index()\n",
    "\n",
    "# Update main df\n",
    "users_df[\"ratings\"] = user_total_ratings.values\n",
    "users_df = pd.merge(users_df, user_positive_ratings, on=['user_id'], how='left').rename(columns={'rating':'positive_ratings'})\n",
    "users_df = pd.merge(users_df, user_negative_ratings, on=['user_id'], how='left').rename(columns={'rating':'negative_ratings'})\n",
    "\n",
    "# Clean none values\n",
    "users_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gower\n",
    "\n",
    "# Get the gower distance matrix\n",
    "distance_matrix = gower.gower_matrix(users_df.drop(columns=['user_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=shc.ClusterWarning)\n",
    "    # Compute linkage using the distance matrix\n",
    "    linkage = shc.linkage(\n",
    "        distance_matrix,\n",
    "        method='ward'\n",
    "    )\n",
    "\n",
    "# Use fcluster to get the cluster labels\n",
    "# `t` is the threshold to use to cut the dendrogram - higher `t` means less clusters / more data points within individual clusters\n",
    "t = 8\n",
    "clusters = shc.fcluster(linkage, t, criterion='distance')\n",
    "\n",
    "# get unique cluster labels\n",
    "unique_labels = np.unique(clusters)\n",
    "\n",
    "# Adding the results to a new column in the dataframe\n",
    "users_df[\"cluster_shc\"] = clusters\n",
    "\n",
    "print(f'Generated {len(unique_labels)} clusters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter(users_df.cluster_shc.to_list())\n",
    "counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize clusters (theoretical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a t-SNE object\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30, # [5, 50] default is 30\n",
    "    learning_rate=200, # [10.0, 1000.0] , def=200\n",
    "    n_iter=1000, # >250, def=1000\n",
    "    metric=\"euclidean\"\n",
    ")\n",
    "\n",
    "# Perform t-SNE on the distance matrix\n",
    "tsne_data = tsne.fit_transform(distance_matrix)\n",
    "# test = tsne.fit(distance_matrix)\n",
    "\n",
    "# Plot the t-SNE data using a scatter plot\n",
    "plt.scatter(tsne_data[:, 0], tsne_data[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cluster labels to the t-SNE data\n",
    "tsne_data_clusters = np.column_stack((tsne_data, clusters))\n",
    "\n",
    "# Plot the t-SNE data using a scatter plot\n",
    "plt.scatter(tsne_data_clusters[:, 0], tsne_data_clusters[:, 1], c=tsne_data_clusters[:, 2], cmap='Spectral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the gower distance matrix\n",
    "distance_matrix_2 = gower.gower_matrix(users_df.drop(columns=['user_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=shc.ClusterWarning)\n",
    "    # Compute linkage using the distance matrix\n",
    "    linkage = shc.linkage(\n",
    "        distance_matrix_2,\n",
    "        method='ward'\n",
    "    )\n",
    "\n",
    "# Use fcluster to get the cluster labels\n",
    "# `t` is the threshold to use to cut the dendrogram - higher `t` means less clusters / more data points within individual clusters\n",
    "t = 4.2\n",
    "clusters = shc.fcluster(linkage, t, criterion='distance')\n",
    "\n",
    "# get unique cluster labels\n",
    "unique_labels = np.unique(clusters)\n",
    "\n",
    "# Adding the results to a new column in the dataframe\n",
    "users_df[\"group_clusters\"] = clusters\n",
    "\n",
    "print(f'Generated {len(unique_labels)} clusters.')\n",
    "counter = collections.Counter(users_df.group_clusters.to_list())\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from models.lightgcn.train_clusters_script import train_on_groups\n",
    "\n",
    "# Baseline serendipity calculated from a normal train in a previous experiment on the dataset\n",
    "baseline_serendipity = pd.read_csv('./output/exp-3/baseline_serendipity.csv')\n",
    "\n",
    "iterations = np.linspace(4, 6, 20)\n",
    "for i in iterations:\n",
    "    clusters = shc.fcluster(linkage, i, criterion='distance')\n",
    "    # get unique cluster labels\n",
    "    unique_labels = np.unique(clusters)\n",
    "\n",
    "    # Total groups obtained (groups of clusters)\n",
    "    print(len(unique_labels))\n",
    "\n",
    "    # Save clusters in df\n",
    "    users_df[\"group_clusters\"] = clusters\n",
    "\n",
    "    # Train model and check serendipity per group\n",
    "    new_user_serendipity = train_on_groups(users_df)\n",
    "\n",
    "    # Apply condition (if we increased serendipity per 90% of groups stop and save i)\n",
    "    serendipity_df = baseline_serendipity.merge(new_user_serendipity, on='userID')\n",
    "    serendipity_df['comparison'] = serendipity_df.apply(lambda x: 1 if (x.user_serendipity_y > x.user_serendipity_x) else 0, axis=1)\n",
    "    total_users = len(serendipity_df)\n",
    "    total_hits = sum(serendipity_df.comparison.to_list())\n",
    "\n",
    "    threshold = total_hits/total_users * 100\n",
    "    print(threshold)\n",
    "    serendipity_df.to_csv('./output/exp-3/group_iterations/' + str(i) + '.csv', index=False)\n",
    "\n",
    "    if threshold > 80:\n",
    "        print(\"Threshold achieved at\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "serendipity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
